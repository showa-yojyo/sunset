---
title: 322 日目（晴れ）妙に長い一日だった
---

[ウルファールのサンプルゲーム with DTC][bshf21b] ホーリィとウルフでメルホース撃破。
前にも書いたが、単純打撃しかしない武器を避ければ直接攻撃でダメージが入る。そこは注意する。
MP 回復アイテムは不要だった。その代わり生命の水をジャブジャブ使った。
これも戦闘の最後の方からゲームオーバーまで撮影。やはりこういう検証は面白い。

日記をプッシュするついでに Twitter を少し見る。

ゲームに戻る。マールマリィとココに交代。試しにそのへんのザコと戦ってみる。
ポイズンジャイアント＆アムルアムト戦くらいで全く歯が立たない。先が思いやられる。

いつの間にか寝床で眠っていて 9:20 起床。洗濯機をまわしながら朝食。納豆をポロポロこぼす。
洗濯物を干して PC かばんを持って外出。図書館へ急ぐ。

八広図書館。朝刊（産経、東京）を読んで入館。この好天で日曜のこの時間帯だがキャレルに空きあり。

* [都内入浴料金の推移 &vert; 【公式】東京銭湯／東京都浴場組合](https://www.1010.or.jp/guide/%e9%83%bd%e5%86%85%e5%85%a5%e6%b5%b4%e6%96%99%e9%87%91%e3%81%ae%e6%8e%a8%e7%a7%bb/): 教材候補
* [【新作VR音ゲー】BEAT ARENA 体験配信！【ビートアリーナ / DOLCE.】 - YouTube](https://www.youtube.com/watch?v=Zl-rdmU_fWY):
  図書館で視聴する回ではなかった。DOLCE プロのベース演奏パフォーマンスで涙が出る。VR 恐るべし。

午後になって退館。イトーヨーカドー曳舟店で体温を見る。

13:30 曳舟の部屋に戻る。玄関周りで異臭がする。扉を開けて換気扇を回す。
PC を戻しておやつ休憩。というか、すでに眠い。

14:10 麻雀の練習を終える。眠くならぬうちに外出するというのはどうだ？
そうしよう。リュックを背負って出発する。スカイツリータウンに移動してトイレを済ませる。
地下鉄に潜って押上駅から日本橋駅へ移動する。何か乗り過ごしたようだ。いったん改札を出ないと反対車線に移動できないのか。切符の人はアウツ。

日本橋駅から東日本橋駅へ移動して正しく乗り換える。小川町駅で降りて白泉社コースで外神田へ。
雑誌を読みたいので昌平まちかど図書館に移動。しかし、なんと今日は休館日だ。日曜に休館日があるのをうっかりしていた。時間が余ってしまった。

仕方がないので秋葉原 HEY 二階に移動。イルベロを 2 プレイ。今日は調子が良さそうだ。

17:20 小諸そば昌平橋店。380 円。二枚盛り大盛り。

再び HEY に戻ってイルベロやビートマニア。今日は最終的に 7 クレ遊ぶ。
イルベロは 93 億と 90 億をハイスコア画面に入れて、これで本当に 90 億以上獲らないとトップ 10 になれない。

ビートマニアは ARENA と STANDARD をそれぞれ 1 ゲームずつ。
Route 80s ハメは十段プレイヤーには効くことを確認。あとレベル 11 譜面 Clione のクリアランプ点灯。

何時に店を出たか忘れた。小川町駅から押上駅に移動。地下道を歩いて業平側に出て買い物に行く。

21:05 ビッグエー墨田業平店。491 円。ついに調味料に手を出す。
醤油を選ぶのが無難なのだが、値段としてはポン酢だろう。どうせ豆腐しか食わないし。

* 絹豆腐
* 国産ゆずぽん酢 (360)
* 大きなおむすび鮭
* ソーセージパン
* パンケーキメープル＆マーガリン
* ドデカイラーメンチキン

21:25 曳舟の部屋に戻る。入浴。なんとなく出る。豆腐をゆずぽんで食う。PC に戻ってゆっくり過ごす。
携帯電話の充電やテザリングを交互に行う。インターネットは電車でたっぷり見たのだった。

23:05 図書館で途中で止めていた Scrapy のスパイダー機能のノートを片付ける。
もうゲームに行く。[ウルファールのサンプルゲーム with DTC][bshf21b] の条件付きリオーネ戦に挑む。
つまりマールマリィとココの二人で戦うのだ。これはいかにもキツイ。

23:20 二度目の戦闘で勝つ。武器と防具は素直なものを選ぶ。道具袋を二人に持たせる。
開幕で賢者の魔術球で固めて、生命の水をジャブジャブ使ってココの氷のブレス頼みという作戦しかない。
リオーネが高めの確率で凍結してくれるので、その間に体勢を立て直す。

これもゼロ人パーティー動画を撮影しておく。あと一戦。
しかし今晩の残り時間はダラダラとニワトリの経験値を上げておく。
パーティーをロズモンド・ビビアン・夕一にする。
そういえばロズモンドとビビアンそれぞれの技が前作から一部削られているのだった。雑魚戦が微妙に長引く。

夕一のレベルを 84 にする。次のレベルアップまで 60 何万とか。

## Scrapy Note

### スパイダー

Scrapy ではスパイダーをクラスで表す。
特定のウェブサイトを這い回っていろいろなページから欲しいデータをかき集める方法を指定するものだ。

スパイダーには反復手順とでもいうようなものがあり、だいたい次のようになる：

1. 最初の URL を這いずり回るべく、リクエストを生成することから始める。
   そのリクエストからダウンロードされた応答を処理する関数を指定する。

   * これはメソッド `start_requests()` の呼び出しでなされる。
   * URL を `start_urls` に指定する。形式はテンプレコードを参照。

2. コールバック関数では応答すなわちウェブページを分析して、文字列分析したアイテムオブジェクトを返したり、
   `Request` オブジェクトを返したり、そういうオブジェクトの iterable を返したりする。
   ここで返した `Request` オブジェクトがまた（それらが指定する）コールバックに応答が到着する。

   コールバック関数ではふつうは `Selector` を利用してページの内容を分析する。
   それから加工したデータをアイテムとして `yield` する。

3. 最後に、スパイダーから返されるアイテムを、ふつうはデータベースに保存したり、ファイルに出力したりする。

私が Scrapy を使い始めた当初のハードルは上記の 1. と 2. だ。
リクエストと文字列処理の連携が非同期的だというのがわかっていなくて、MJ.NET のページ遷移で失敗しまくっていた。

#### クラス `Spider`

クラス `Spider` がいちばん単純なスパイダーだ。
上述した `start_urls` と `start_requests()` の連携する既定の実装しか与えない。

主要なプロパティーを表にする：

| name | description | comment |
|------|-------------|---------|
| `name` | スパイダーの名前 | `genspider` で決まる |
| `allowed_domains` | 這いずり回ることを認めるドメイン | リストで指定 |
| `start_urls` | 這いずり回る URL の始点 | リストで指定 |
| `logger` | Python 標準のログ機能 | `self.logger.info(...)` のように使う |

主要なメソッドを表にする：

| name | description | comment |
|------|-------------|---------|
| `start_requests()` | スパイダーが這い回るための `Request` の iterable を返す | ジェネレーターとして書くのが無難 |
| `parse(response)` | 応答を処理する既定のコールバック | 応答を処理してデータか URL を返す |

* `start_urls` を明示的に設定してある場合、`start_requests()` を実装せずに済ませることができる。
  反対に、`start_requests()` を実装して `start_urls` を無視するということもできる。

#### スパイダーに引数を渡す

コンソールからコマンド `crawl` や `runspider` を実行するときにオプション `-a KEY=VALUE` でスパイダーに引数を渡せる。

```console
bash$ scrapy crawl MYSPIDER -a KEY1=VALUE1 -a KEY2=VALUE2 ...
```

* スパイダークラスで `def __init__(self, KEY=None, *args, **kwargs)` のように書くか、
* メソッド内で `self.KEY` の形式で参照する。ただしコマンドラインで指定されていない場合には例外が送出する。

TODO: 次のオプションは別途処理される？

* `http_user`
* `http_pass`
* `user_agent`

#### 一般的なスパイダー

* `CrawlSpider`: これがふつうのウェブサイドをクロールするのに用いられるスパイダー。
  * プロパティー `rules` に基づいてクロールするページが決まる。
    これは `Rule` オブジェクトのリスト。
  * メソッド `parse_start_url()` をオーバーライドすることがあるかもしれない。
* `XMLFeedSpider`: その名の示すとおりのものをクロールする。クロールというのか？
  * `itertag` を指定。その上でメソッド `parse_note()` をオーバーライドする。
  * 引数 `response` の次の引数が XML のノードを表す。これは `Item` オブジェクトを生成して返す。
* `CSVFeedSpider`: 上記スパイダーの CSV 版。`delimiter`, `quotechar`, `headers` などを指定。
  * メソッド `parse_row()` をオーバーライドする。引数 `row` は辞書オブジェクト。
* `SitemapSpider`: sitemap.xml や robots.txt をクロールするためのスパイダー。

重要なのはクラス `CrawlSpider` だ。

スパイダークラス `CrawlSpider` の仕組みを理解するのにクラス `Rule` を理解する。
これはコンストラクターの引数リストから察せられるように、ページ内の URL と処理規則とを結合する役を果たす。
一部を示す。

| parameter | description | comment |
|-----------|-------------|---------|
| `link_extractor` | クロール対象である URL を抽出する `LinkExtractor` オブジェクト | 後述 |
| `callback` | 抽出されたリンクを処理する callable | そのような callable は `Response` オブジェクトを引数に取る |
| `cb_kwargs` | 上記 callable のキーワード引数となる `dict` オブジェクト | |
| `follow` | 抽出されたリンク先にジャンプするか否かを表す `bool` 値 | |
| `process_links` | 抽出されたリンクのリストをフィルターするための callable | |
| `process_request` | 抽出された `Request` オブジェクトを処理する callable | これもフィルターのように実装する |

なお、クラス `LinkExtractor` の説明はもっと後でするのだが、これ単体で有用なので早く述べてもいいだろう。

[bshf21b]: https://wodifes.net/game/show/446
