---
title: 321 日目（雨のち曇り）あんなに巨大な避雷針があるではないか
---

[ウルファールのサンプルゲーム with DTC][bshf21b] の条件付きメルホース戦をもう一度挑戦。
結論から言うと道具の使用をしていいのなら、不可能ということはなさそうだ。
ホーリィの MP 切れを克服する必要がある。序盤のお供 B を倒すまでの消耗戦がキツイ。

1:00 就寝。ダメな日はぐっすり眠れる。8:35 に一度起き上がってトイレに行くなどする。

9:00 にあらためて起床。納豆とロールパンを頬張る。これくらいの量の炭水化物がないと納豆の味を包み込みきれないのか。
雨が降っているので面接用の靴を履いて外出。行き先は図書館だが。

雨が小降りになっていて助かる。9:40 八広図書館到着。朝刊（産経）を読んで入館。空席だらけでありがたい。

PC を机上に広げ、いつものファイル更新・同期。YouTube を聴きながら Scrapy を調べる。
RSS を解析するのに特化したスパイダークラスで時間を食われる。サンプルコードを真似して東京銭湯のフィードを解析しようとしたらセレクターが何も返さない。
これの原因を突き止めるのが難しかった。XML の名前空間に対する特殊な処理が要る。
今までは HTML しかスクレイプしていなかったから気づきにくかった。

12:10 退館。雨がだいたいやんでいる。イトーヨーカドー曳舟店に寄って体温チェック。

12:50 曳舟の部屋に戻る。PC を戻しておやつ休憩。さっき食ったばかりではないか。
麻雀の練習をしていると雨が激しくなる。Scrapy の調べ物を少しして居眠り。

16:00 起き上がる。やはり降っているのだが外出する。
雷が鳴っているような天気だが、さいわいここはスカイツリーの近傍なので落雷の心配はゼロ。
押上駅のバス停に着いた途端にバスが来る。乗車して太平四丁目で降りる。

16:45 タイトー F ステーションオリナス錦糸町店に入店。こんな天気なのに混雑していて安心だ。
MJ の筐体が満席なので、ブラブラして時間を潰す。

隣の島の麻雀格闘倶楽部でプレイヤーがどこかへ行った席がある。勝手に代走する。コカコーラが置きっぱなしでなっていない。
どうやらゲストプレイで、東風戦東二局。上家の親が連荘中らしい。
こちらはさっきまでツモ切りマシーンだったというのに、下家と対面の点棒状況がボロボロ。
テキトーに親を流してオーラスまでダラダラ時間稼ぎ。オーラスでは 1000 点アガって二着に逆転して終局したい。
すると下家と対面が逆襲リーチ。いつの間にかトップ目が CPU になっていて、対面に振り込みやがって私の逆転ラスで終了。

気を取り直して MJ プロ卓東風戦。やはり悪い雰囲気は払拭できなかった。
というか、認定雀士が二名同卓するような卓でどう勝てるというのだ。一発入れることすら叶わなんだ。

```text
【SCORE】
合計SCORE:-87.2

【最終段位】
四人打ち段位:魔神 幻球:6

【3/13の最新8試合の履歴】

1st|----**--
2nd|-------*
3rd|-*----*-
4th|*-**----
old         new

【順位】
1位回数:2(22.22%)
2位回数:1(11.11%)
3位回数:3(33.33%)
4位回数:3(33.33%)
平均順位:2.78

プレイ局数:47局

【打ち筋】
アガリ率:21.28%(10/47)
平均アガリ翻:3.50翻
平均アガリ巡目:10.80巡
振込み率:12.77%(6/47)

【3/13の最高役】
最高役のデータがありません。最高役は、跳満以上のアガリが対象となります。
```

ビートマニア ARENA モードを CPU と遊んで退店。

19:50 カスミオリナス錦糸町店。405 円。クーポン使用。

* オクラ茄子丼
* ビーフメンチカツ
* ブラックチョコ
* ライス

20:15 ビッグエー墨田業平店。247 円。

* 絹豆腐
* コッペパン和栗＆ホイップ
* 大きなツナオニオンパン
* ポテコうましお

20:30 曳舟の部屋に戻る。入浴。そういえば外は雨天のわりには寒くなかった。

21:00 風呂から出て PC に張り付く。豆腐を食いながら帳簿つけ。日記を進める。晩飯。
麻雀の練習。テザリング。

22:55 Scrapy のコマンドラインツールのドキュメントを一通り把握する。
ここをざっと見た上で、未知の概念がコマンドラインオプションを通していくつかあることを頭の片隅に留めておく。

[ウルファールのサンプルゲーム with DTC][bshf21b] をやる。
しつこくメルホース戦。お供 B の呪文を完封できないのがわかったので速攻を試みる。
MP が枯渇しそうだが、やはりホーリィのビリビリは有効にしておく。
ウルフによるデスマーク＆バックスタブで B を倒せるか試す。武器に工夫が要るものの上手くいった。

あとは A を倒す。ここからは二連ファイヤーを多用する。ホーリィは可能な限り聖歌。
それ以外のターンはチリングタッチやら死んだウルフの回復やらをするしかない。

## Scrapy Note

### Command line tool

TODO: `scrapy.cfg` の概要

コンソールから `scrapy COMMAND options args` のようにして起動するコマンドの一覧。
プロジェクトが必要なものとそうでないものとに分類できる（ここには記さない）。

| command | description | comment |
| ------- | ----------- | ------- |
| bench        | Run quick benchmark test | 今は詳しく知らなくてよい |
| check        | Check spider contracts commands | 自分の書いたスパイダーをある調べる |
| crawl        | Run a spider | スパイダーをふつうに実行 |
| edit         | Edit spider | テキストエディターでモジュールを開く |
| fetch        | Fetch a URL using the Scrapy downloader | ふつうは応答の本体を見るがヘッダーを見ることもできる |
| genspider    | Generate new spider using pre-defined templates | テンプレートからモジュールを生成 |
| list         | List available spiders | ここにスパイダークラスの `name` が現れる |
| parse        | Parse URL (using its spider) and print the results | `parse()` 相当を指示できる |
| runspider    | Run a self-contained spider (without creating a project) | スパイダークラスを定義したモジュールだけで実行できる |
| settings     | Get settings values | cfg ファイルで設定されている内容を確認する |
| shell        | Interactive scraping console | IPython があればそれが走る |
| startproject | Create new project | フレームワークに乗るディレクトリー構造を生成する |
| version      | Print Scrapy version | 付随するライブラリーのバージョンも出力できる |
| view         | Open URL in browser, as seen by Scrapy |  |

* `scrapy crawl SPIDER` がふつうの使い方だ。
  * `SPIDER` はプロジェクトにあるスパイダークラスの `name` を指定する。これは `list` で確認することもできる。
* `scrapy edit SPIDER` は対応するファイルを既定のエディターで開くだけ。自分でエディターをコンソールから入力するのと手間はほとんど変わらないだろう。
* `scrapy fetch --nolog --headers URL` でヘッダーだけを得られる。
* `scrapy genspider SPIDER_NAME ALLOWED_DOMAIN` の形式で実行するとスパイダークラスのための Python ファイルを生成する。
  * `--list`, `-l` でスパイダーのテンプレの名前を一覧する。
    * `basic`
    * `crawl`
    * `csvfeed`
    * `xmlfeed`
  * `--dump=TEMPLATE`, `-d TEMPLATE` でテンプレの定義を標準出力に表示できる。
  * `--template=TEMPLATE`, `-t TEMPLATE` でスパイダークラスのテンプレを指定する。
  * `--edit`, `-e` でファイル生成後にそれをエディターを開く。
* `scrapy parse [options] <url>` はプロジェクトにあるスパイダーを使って文字列に関する分析をする。
  オプションを指定して特別なことをするのに利用する。
  * `-a NAME=VALUE` でスパイダーに引数を渡す。これは複数あってよい。
    コードではスパイダークラスのメソッドで `self.NAME` の形式で `VALUE` を参照する。
  * `--output=FILE`, `-o FILE`: スクレイプしたデータを出力するファイルを指定する。
    * これらは append モードで動く。
    * 標準出力は `FILE` として `-` を指定する。
  * `--overwrite-output=FILE`, `-O FILE`: 上記オプションの write 版。
  * `--output-format=FORMAT`, `-t FORMAT`: 出力書式を指定する。
    * ただし出力先が標準出力の場合には妙な例外が送出されてダメ。
    * TODO: 有効な FORMAT は後述。
  * `--spider=SPIDER`: スパイダー `SPIDER` を用いるようにする（クラスの定義から自動検出されるものではなく）。
  * `--callback CALLBACK`, `-c CALLBACK`: 応答を解釈するコールバックメソッドを指定する。
  * `--nolinks` で抽出したリンクを出力しないようにする。
  * `--noitems` で抽出したものを出力しないようにする。
* `scrapy runspider myspider.py` とすると、プロジェクトを作る必要がないスパイダーを実行できる。
* `scrapy shell [URL|FILE]` が基本形。
  * `scrapy shell -c CODE` でコードを実行。例えば：
    `scrapy shell --nolog http://www.example.com/ -c '(response.status, response.url)'`
* `scrapy startproject <project_name> [project_dir]` を実行すると Scrapy が扱えるディレクトリー構造を生成する。
* `scrapy view <url>` でブラウザーが開くことになっている。
  * [WSL の Python だと動かない](https://github.com/scrapy/scrapy/issues/4589)。

TODO: カスタムコマンドの実装方法（余裕があれば）

[bshf21b]: https://wodifes.net/game/show/446
